# Implementation Plan: LLM Pipeline Fix & Refactor

**Branch**: `002-helper-llm-feature` | **Date**: 2025-12-03 | **Spec**: [spec.md](spec.md)

## Summary

This plan addresses critical bugs and architectural deficiencies in the LLM-based data enrichment pipeline. The primary goal is to make the feature functional, robust, and cost-effective. The work involves two main thrusts:
1.  **Immediate Bug Fix**: Diagnose and resolve the issue causing the LLM to return unusable data, despite successful API calls.
2.  **Architectural Refactoring**: Overhaul the caching and logging systems to improve debuggability, reduce redundant API calls, and increase data traceability.

## Proposed High-Performance Pipeline

This new design introduces a dedicated caching layer for the expensive LLM calls.

**Phase 1:** Web Scraping  
* Fetch the Wikipedia HTML.  
* Cache the raw HTML with a clear, descriptive filename (e.g., `cache/html/Bayside_Queens.html`).

**Phase 2:** LLM Enrichment & Caching  
* Make the API call to the LLM.  
* Upon receiving a successful, valid JSON response, save this data to its own dedicated cache file.  
* The filename will be descriptive and timestamped as you suggested (e.g., `cache/llm/Bayside_Queens_20251203T111655.json`).

**Phase 3:** Generation & Logging  
* The Markdown file is generated by merging the data from the cached LLM response and the initial web scrape.  
* The `generation_log.json` is updated to include a reference to the specific LLM cache file that was used.

### Advantages of this New Architecture
- **Cost-Effective:** We only call the LLM API once for each neighborhood. Subsequent runs will use the cached response, saving time and money.
- **Debuggable:** If a profile looks wrong, we can inspect the exact LLM JSON that was used to create it by looking at the file referenced in `generation_log.json`.
- **Robust:** We can re-run the Markdown generation from the LLM cache anytime without needing to hit the live API again.  
- **Organized:** The file naming convention makes the whole system transparent and easy to understand.


## Technical Context

**Language/Version**: Python 3.11
**Primary Dependencies**: `requests`, `beautifulsoup4`, `pydantic`, `typer`, `openai`
**Storage**: Filesystem for input/output, logs, and a **new, two-tiered cache system** for raw HTML and structured LLM responses.
**Generation Log**: The existing JSON log (`generation_log.json`) will be enhanced.
**Testing**: `pytest`
**Constraints**: All changes must integrate with the existing data models and Markdown template structure. The refactoring should prioritize clarity and maintainability.

## Constitution Check

- **I. Data Accuracy & Traceability**: This plan **enhances** traceability by introducing a dedicated cache for LLM responses and linking them directly to the final generated profiles via the generation log.
- **IV. Verification & Robustness**: This plan **enhances** robustness by adding the LLM response cache, which makes the pipeline resilient to transient network or API issues on subsequent runs.
- **V. Extensibility & Maintainability**: This plan **enhances** maintainability by creating a more organized, transparent caching system with descriptive filenames, making debugging significantly easier.

## Plan & Task Breakdown

### Phase 1: Diagnosis & Immediate Bug Fix

1.  **Diagnose the LLM Response**: Execute the script with `--log-level DEBUG` to capture the raw response content from the LLM API call.
2.  **Fix LLM Response Handling**: Based on the diagnosis, modify `src/services/llm_helper.py` to correctly parse the LLM's response into a valid JSON object.

### Phase 2: Architectural Refactoring

3.  **Refactor Caching Architecture**:
    -   Modify `src/lib/cache_manager.py` to support descriptive filenames and distinct cache directories (e.g., `cache/html/`, `cache/llm/`).
    -   Update `src/services/web_fetcher.py` to use the refactored cache for HTML content.
    -   Implement logic in `src/services/llm_helper.py` to save successful, parsed LLM responses to `cache/llm/` with a descriptive, timestamped filename.

4.  **Enhance Generation Log**:
    -   Update the data model for the log entry to include a new optional field: `llm_cache_path: Optional[str]`.
    -   Modify `src/lib/generation_log.py` to handle reading and writing this new field.

5.  **Update Orchestration Logic**:
    -   Modify `src/services/profile_generator.py` and `src/services/llm_helper.py` to create a new workflow:
        a. Before making a live API call, check if a valid LLM cache file is referenced in the generation log or exists at the expected path.
        b. If a valid cache file exists and `--force-regenerate` is not used, load the data from that file instead of calling the LLM API.
        c. When a new profile is generated via a live API call, the `ProfileGenerator` must record the path to the newly created LLM cache file in the generation log.

### Phase 3: Verification

6.  **Final Verification**: Run the complete, refactored pipeline. Confirm that:
    -   Profiles are generated with enriched LLM data.
    -   HTML and LLM cache files are created with the correct names and content.
    -   The `generation_log.json` is updated with the `llm_cache_path`.
    -   Re-running the script does *not* trigger new LLM API calls for existing profiles (verified via logs).
